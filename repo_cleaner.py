#!/usr/bin/env python3
import os
import re
import sys
import argparse
import random
import subprocess
import shutil
import json  # For package.json edits
from pathlib import Path
from datetime import datetime  # For backups
import time  # For ETA
import signal  # For daemon graceful shutdown
import threading  # For daemon monitoring
from tqdm import tqdm  # For dynamic progress bar
# Optional: daemonize (pip install python-daemon if needed)
try:
    import daemon
    DAEMON_AVAILABLE = True
except ImportError:
    DAEMON_AVAILABLE = False
    print("Warning: 'python-daemon' not installed—daemon mode limited to threaded loop.")

# Dynamic Patterns for Lovable and other AI watermarks (case-insensitive) - Configurable via JSON or extend here
def load_patterns(pattern_file=None):
    """Dynamic pattern loader: Default + file override for evolving watermarks."""
    default_patterns = [
        r'(?i)(lovable|lovable\.dev|edit with lovable|generated by lovable|don\'t delete this lovable)',
        r'(?i)#lovable-badge',
        r'(?i)class\s*=\s*["\']lovable[^"\']*["\']',
        r'(?i)id\s*=\s*["\']lovable[^"\']*["\']',
        r'(?i)src\s*=\s*["\'][^"\']*lovable[^"\']*\.svg["\']',
        r'(?i)useLovableEdit|lovable-export:\s*true',
        r'(?i)lovable\.com|powered by lovable',
        r'(?i)"lovable-.*?"',
        # Extended for other AI watermarks (dynamic growth)
        r'(?i)(cursor|cursor\.sh|generated by cursor|ai-powered by cursor)',
        r'(?i)(replit|replit\.com|replit ghost|ai assistant replit)',
        r'(?i)(github copilot|copilot generated|copilot: true)',
        r'(?i)(claude|anthropic|claude-ai|generated with claude)',
        r'(?i)(gpt|openai|chatgpt|generated by gpt)',
    ]
    if pattern_file and Path(pattern_file).exists():
        try:
            with open(pattern_file, 'r') as f:
                custom = json.load(f).get('patterns', [])
            default_patterns.extend(custom)
            print(f"✓ Loaded dynamic patterns from {pattern_file}: {len(custom)} extras")
        except:
            print(f"⚠ Pattern file {pattern_file} invalid—using defaults.")
    return default_patterns

LOVABLE_PATTERNS = load_patterns()  # Initialize dynamically
# Neutral replacements for natural look
DECOYS = [
    "CustomForge", "DevPhantom", "CodeVoid", "AnonBuilder", "ShadowGen", "UI-Optimizer",
    "// Perf-tuned render", "/* Legacy UI hook */", "#app-badge { display: none; }",
    '"export-tool": "custom"', '"generator": "local"'
]
# Secure file delete (global scope)
def shred_file(fp: Path, verbose=False):
    try:
        if os.name == 'nt':
            subprocess.run(['cipher', '/w:' + str(fp)], shell=True, capture_output=True)
        else:
            os.system(f"shred -u -z -n 3 {fp} 2>/dev/null || true")
        if verbose:
            print(f" - Deleted: {fp.name}")
    except:
        pass

def quick_sentinel_scan(root_path: Path, patterns, verbose=False) -> bool:
    """Dynamic scan for watermarks—halt if none found, with message."""
    watermark_types = set()
    for dirpath, _, filenames in os.walk(root_path):
        for filename in filenames:
            file_path = Path(dirpath) / filename
            ext = file_path.suffix.lower()
            if ext in {'.html', '.js', '.jsx', '.ts', '.tsx', '.css', '.mdx', '.md', '.txt', '.json', '.svg', '.yaml', '.yml', '.toml', '.config'}:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content_lower = f.read().lower()
                        for pattern in patterns:
                            if re.search(pattern, content_lower):
                                # Extract type from pattern for message
                                if 'lovable' in pattern.lower():
                                    watermark_types.add('Lovable')
                                elif 'cursor' in pattern.lower():
                                    watermark_types.add('Cursor')
                                elif 'replit' in pattern.lower():
                                    watermark_types.add('Replit')
                                elif 'copilot' in pattern.lower():
                                    watermark_types.add('Copilot')
                                elif 'claude' in pattern.lower():
                                    watermark_types.add('Claude')
                                elif 'gpt' in pattern.lower():
                                    watermark_types.add('GPT')
                                if verbose:
                                    print(f" - Watermark '{watermark_types.pop()}' detected in {file_path.name}—proceeding.")
                                return True
                except UnicodeDecodeError:
                    continue
    if not watermark_types:
        print("No AI watermarks (Lovable, Cursor, Replit, Copilot, Claude, GPT, etc.) found—repo pristine. No action taken to preserve void.")
        return False
    return True

def rename_lovable_branches(root_path: Path, dry_run=False, verbose=False, log_entries=None):
    """Rename Git branches containing watermark references to neutral decoys."""
    if dry_run:
        print("--- DRY-RUN: Branch rename preview ---")
        return []
    
    try:
        # Get all branches
        result = subprocess.run(['git', 'branch', '--list', '-a'], cwd=root_path, capture_output=True, text=True, check=True)
        branches = [b.strip().replace('remotes/origin/', '').replace('* ', '') for b in result.stdout.splitlines() if b.strip()]
        
        renamed_count = 0
        renames = []
        for branch in branches:
            if re.search(r'(?i)(lovable|cursor|replit|copilot|claude|gpt)', branch):
                # Generate neutral name: e.g., 'lovable-dev/bot' -> 'ShadowGen-dev/bot'
                neutral_name = re.sub(r'(?i)(lovable|cursor|replit|copilot|claude|gpt)', random.choice(['CustomForge', 'DevPhantom', 'CodeVoid']), branch)
                if verbose:
                    print(f" - Renaming branch '{branch}' to '{neutral_name}'")
                if not dry_run:
                    # Switch to branch if current, rename, switch back if needed
                    current = subprocess.run(['git', 'branch', '--show-current'], cwd=root_path, capture_output=True, text=True).stdout.strip()
                    if current == branch:
                        subprocess.run(['git', 'checkout', '-b', neutral_name], cwd=root_path, check=True, capture_output=True)
                        subprocess.run(['git', 'branch', '-D', branch], cwd=root_path, check=True, capture_output=True)
                    else:
                        subprocess.run(['git', 'branch', '-m', branch, neutral_name], cwd=root_path, check=True, capture_output=True)
                    renamed_count += 1
                    renames.append({'old': branch, 'new': neutral_name})
        
        if renamed_count > 0:
            print(f"✓ Renamed {renamed_count} watermark-tainted branches to neutral voids.")
            if log_entries is not None:
                log_entries.append({'event': 'branch_rename', 'count': renamed_count, 'details': renames})
        return renames
    except subprocess.CalledProcessError:
        if verbose:
            print("⚠ Git branch ops failed—manual rename advised.")
        return []

def scrub_file(file_path: Path, dry_run=False, mutate=False, verbose=False, log_entries=None, obfuscate=False, patterns=LOVABLE_PATTERNS) -> list:
    """Clean one file for watermark references dynamically."""
    if not file_path.is_file():
        return []
  
    ext = file_path.suffix.lower()
    if ext not in {'.html', '.js', '.jsx', '.ts', '.tsx', '.css', '.mdx', '.md', '.txt', '.json', '.svg', '.yaml', '.yml', '.toml', '.config'}:
        return []
  
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except UnicodeDecodeError:
        return []
  
    original_content = content
    changes = []
    match_count = 0
    for pattern in patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
        if matches:
            match_count += len(matches)
            if '//' in content or '/*' in content or '#' in content or ext in {'.css', '.svg', '.yaml', '.toml'}:
                content = re.sub(pattern, '', content, flags=re.MULTILINE | re.DOTALL)
            else:
                content = re.sub(pattern, '', content)
  
    if match_count > 0:
        changes.append(f"File {file_path.name}: {match_count} references removed")
        if verbose:
            print(f" - Hit in {file_path.name} (pre: {len(original_content)} chars)")
        if log_entries is not None:
            log_entries.append({'file': str(file_path), 'matches': match_count, 'pre': len(original_content)})
  
    if mutate and random.random() < 0.10:
        decoy = random.choice(DECOYS)
        if ext == '.json':
            try:
                data = json.loads(content)
                def deep_nuke(obj):
                    if isinstance(obj, dict):
                        for k, v in list(obj.items()):
                            if any(re.search(p, str(k) + str(v), re.IGNORECASE) for p in patterns):
                                obj[k] = decoy if isinstance(v, str) else {"neutral": decoy}
                            else:
                                deep_nuke(v)
                        return obj
                    elif isinstance(obj, list):
                        return [deep_nuke(item) for item in obj]
                    return obj
                data = deep_nuke(data)
                content = json.dumps(data, indent=2)
            except json.JSONDecodeError:
                pass
        else:
            for _ in range(min(3, sum(content.lower().count(w) for w in ['lovable', 'cursor', 'replit', 'copilot', 'claude', 'gpt']))):
                pos = content.lower().find(random.choice(['lovable', 'cursor', 'replit', 'copilot', 'claude', 'gpt']), random.randint(0, len(content)//2))
                if pos != -1:
                    content = content[:pos] + decoy[:len('lovable')] + content[pos+7:]
  
    if changes and not dry_run:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"✓ Cleaned {file_path.name} ({match_count} refs)")
        if verbose:
            print(f" - Post: {len(content)} chars (delta: {len(original_content) - len(content)})")
        if log_entries is not None:
            log_entries[-1]['post'] = len(content)
            log_entries[-1]['delta'] = len(original_content) - len(content)

    # Obfuscation integration (Multi-Tool: Black Magic Obfuscator style)
    if obfuscate and not dry_run and ext in {'.js', '.jsx', '.ts', '.tsx', '.css'}:
        try:
            if ext in {'.js', '.jsx', '.ts', '.tsx'}:
                # Assume uglify-js or similar installed globally
                result = subprocess.run(['uglifyjs', str(file_path), '-o', str(file_path), '--mangle', '--compress'], 
                                        capture_output=True, check=True)
            elif ext == '.css':
                # Use cssnano or cleancss
                result = subprocess.run(['cleancss', '-o', str(file_path), str(file_path)], 
                                        capture_output=True, check=True)
            if verbose:
                print(f" - Obfuscated {file_path.name}: {len(original_content) - len(open(file_path).read())} chars reduced")
            changes.append(f"Obfuscated {file_path.name}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            if verbose:
                print(f"⚠ Obfuscator tool missing for {ext}—install uglifyjs/cleancss globally.")
  
    return changes

def vulnerability_scan(root_path: Path, dry_run=False, verbose=False, auto_patch=False, log_entries=None):
    """Vulnerability Scanner Tie-In: Run lightweight scans and optional auto-patches."""
    if dry_run:
        print("--- DRY-RUN: Vulnerability scan preview ---")
        return []
    
    vulns_found = []
    py_files = list(root_path.rglob('*.py'))
    js_dirs = [d for d in root_path.rglob('*') if (d / 'package.json').exists()]
    
    # Python: Bandit scan
    if py_files and verbose:
        print("Scanning Python for vulns...")
    for py_file in py_files:
        try:
            result = subprocess.run(['bandit', '-f', 'json', '-r', str(py_file.parent)], capture_output=True, text=True)
            if result.returncode == 0:
                data = json.loads(result.stdout)
                issues = [issue for issue in data.get('results', []) if issue.get('issue_severity') in ['HIGH', 'MEDIUM']]
                if issues:
                    vulns_found.extend([{'file': issue['filename'], 'issue': issue['issue_text']} for issue in issues])
                    if auto_patch and verbose:
                        # Simple auto-patch example: Remove eval() if detected (extend as needed)
                        with open(py_file, 'r') as f:
                            content = f.read()
                        if 'eval(' in content:
                            content = re.sub(r'eval\s*\(', 'safe_eval(', content)  # Placeholder safe alt
                            with open(py_file, 'w') as f:
                                f.write(content)
                            print(f" - Auto-patched eval in {py_file.name}")
        except (subprocess.CalledProcessError, FileNotFoundError, json.JSONDecodeError):
            if verbose:
                print("⚠ Bandit missing—pip install bandit.")
    
    # JS: npm audit
    for js_dir in js_dirs:
        try:
            if verbose:
                print(f" Auditing JS in {js_dir}...")
            result = subprocess.run(['npm', 'audit', '--json'], cwd=js_dir, capture_output=True, text=True)
            if result.returncode == 1:  # Audit found issues
                data = json.loads(result.stdout)
                high_vulns = [v for v in data.get('vulnerabilities', {}).values() if v.get('severity') in ['high', 'critical']]
                if high_vulns:
                    vulns_found.extend([{'dir': str(js_dir), 'vuln': v['name']} for v in high_vulns])
                    if auto_patch:
                        subprocess.run(['npm', 'audit', 'fix'], cwd=js_dir, capture_output=True)
                        print(f" - Auto-fixed vulns in {js_dir.name}")
        except (subprocess.CalledProcessError, FileNotFoundError, json.JSONDecodeError):
            if verbose:
                print("⚠ npm audit failed—ensure node_modules exists.")
    
    if vulns_found:
        print(f"⚠ {len(vulns_found)} vulnerabilities detected & {'patched' if auto_patch else 'logged'}.")
        if log_entries is not None:
            log_entries.append({'event': 'vuln_scan', 'count': len(vulns_found), 'details': vulns_found})
    else:
        print("✓ No high-severity vulns found.")
    
    return vulns_found

def history_blur(root_path: Path, dry_run=False, verbose=False, log_entries=None):
    """Blur Git logs for natural history."""
    git_dir = root_path / '.git'
    if git_dir.exists():
        log_files = list(git_dir.glob('logs/refs/heads/*.log')) + [git_dir / 'logs/HEAD']
        blurred_count = 0
        for log_file in log_files:
            if log_file.is_file():
                with open(log_file, 'r') as f:
                    logs = f.readlines()
                mutated = []
                for line in logs:
                    mutated.append(re.sub(r'(?i)(lovable|cursor|replit|copilot|claude|gpt).*?(?=\s+\d+)', random.choice(DECOYS), line))
                    if ' ' in line and ':' in line.split()[-1]:
                        ts_part = line.rsplit(' ', 1)[1]
                        mutated[-1] = line.rsplit(' ', 1)[0] + ' ' + re.sub(r'(\d{2}:\d{2})', lambda m: f"{int(m.group(1)[:2])%24:02d}:{(int(m.group(1)[3:])+random.randint(-5,5))%60:02d}", ts_part)
                if not dry_run:
                    with open(log_file, 'w') as f:
                        f.writelines(mutated)
                    blurred_count += 1
                if verbose:
                    print(f" - Blurred {log_file.name}: {len(logs)} lines")
        if blurred_count > 0 and verbose:
            print(f"✓ History blurred: {blurred_count} files")
        if log_entries is not None:
            log_entries.append({'event': 'history_blur', 'blurred': blurred_count})

def daemon_loop(root_path, dry_run, mutate, commit, bundle, backup, verbose, new_dir, log_file, eta, obfuscate, vuln_scan, auto_patch, rename_branches, pattern_file):
    """Cross-Platform Daemon Mode: Threaded loop for real-time scrubbing."""
    def signal_handler(sig, frame):
        print("\nShutting down daemon gracefully...")
        if log_file:
            with open(log_file, 'a') as lf:
                json.dump({'event': 'shutdown', 'timestamp': datetime.now().isoformat()}, lf)
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    patterns = load_patterns(pattern_file)
    cycle = 0
    while True:
        cycle += 1
        if verbose:
            print(f"[Daemon Cycle {cycle}] Scanning {root_path}...")
        try:
            walk_and_scrub(root_path, dry_run, mutate, commit, bundle, backup, verbose, new_dir, log_file, eta, obfuscate, vuln_scan, auto_patch, rename_branches, patterns)
        except Exception as e:
            if verbose:
                print(f"[Daemon Error] {e}—continuing.")
        time.sleep(3600)  # Hourly sweep; configurable via arg if extended

def walk_and_scrub(root_path: Path, dry_run=False, mutate=False, commit=False, bundle=False, backup=False, verbose=False, new_dir=None, log_file=None, eta=False, obfuscate=False, vuln_scan=False, auto_patch=False, rename_branches=False, patterns=LOVABLE_PATTERNS):
    """Main function: Dynamic scan first, then clean/backup/commit/bundle/obfuscate/vuln scan/branch rename if watermarks exist."""
    log_entries = []
    start_time = time.time()
    if log_file:
        log_entries.append({'event': 'start', 'root': str(root_path), 'timestamp': datetime.now().isoformat()})
  
    # Dynamic sentinel check FIRST - only proceed if watermarks exist
    if not quick_sentinel_scan(root_path, patterns, verbose):
        # No action, just message - preserve pristine void
        if log_file:
            with open(log_file, 'w') as lf:
                json.dump(log_entries, lf, indent=2)
            print(f"Log saved: {log_file} (no changes)")
        return
  
    # Watermarks confirmed - cascade actions
    print("Watermarks detected—unleashing full purge cascade...")
  
    if backup:
        backup_name = f"{root_path.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}.zip"
        shutil.make_archive(backup_name, 'zip', root_path)
        print(f"✓ Backup forged: {backup_name}")
        if log_entries:
            log_entries.append({'event': 'backup', 'file': backup_name})
  
    # Branch rename tie-in (if flagged)
    if rename_branches:
        branch_renames = rename_lovable_branches(root_path, dry_run, verbose, log_entries)
  
    # Collect files for dynamic progress bar
    target_files = []
    for dirpath, _, filenames in os.walk(root_path):
        for filename in filenames:
            file_path = Path(dirpath) / filename
            ext = file_path.suffix.lower()
            if ext in {'.html', '.js', '.jsx', '.ts', '.tsx', '.css', '.mdx', '.md', '.txt', '.json', '.svg', '.yaml', '.yml', '.toml', '.config'}:
                target_files.append(file_path)
    
    file_count = len(target_files)
    total_changes = []
    processed = 0
    
    # Dynamic progress bar with tqdm
    with tqdm(total=file_count, desc="Purging Ghosts", unit="file", disable=not verbose) as pbar:
        for file_path in target_files:
            changes = scrub_file(file_path, dry_run, mutate, verbose, log_entries, obfuscate, patterns)
            total_changes.extend(changes)
            processed += 1
            pbar.set_postfix({'changes': len(total_changes)})
            pbar.update(1)
            if eta and processed % 500 == 0:
                elapsed = time.time() - start_time
                eta_sec = (elapsed / processed) * (file_count - processed) if processed > 0 else 0
                pbar.set_postfix({'ETA': f"{int(eta_sec // 60)}m {int(eta_sec % 60)}s", 'changes': len(total_changes)})
  
    if dry_run:
        print(f"\n--- DRY-RUN: {len(total_changes)} references across {file_count} files ---")
        for change in total_changes[:20]:
            print(change)
        if len(total_changes) > 20:
            print("... (truncated)")
        if log_file:
            with open(log_file, 'w') as lf:
                json.dump(log_entries, lf, indent=2)
            print(f"Log saved: {log_file}")
        return
  
    history_blur(root_path, dry_run=False, verbose=verbose, log_entries=log_entries)
  
    # Vulnerability scan tie-in
    if vuln_scan:
        vulns = vulnerability_scan(root_path, dry_run=False, verbose=verbose, auto_patch=auto_patch, log_entries=log_entries)
  
    if commit and total_changes:
        try:
            subprocess.run(['git', 'add', '.'], cwd=root_path, check=True, capture_output=True)
            commit_msg = f"refactor: cleanup optimizations ({random.randint(300,999)})"
            subprocess.run(['git', 'commit', '-m', commit_msg], cwd=root_path, check=True, capture_output=True)
            subprocess.run(['git', 'push'], cwd=root_path, check=True, capture_output=True)
            print(f"✓ Committed to void: {commit_msg}")
            if verbose:
                print(" - Git staged and pushed—trails blurred.")
            if log_entries:
                log_entries.append({'event': 'commit', 'msg': commit_msg})
        except subprocess.CalledProcessError:
            print("⚠ Git failed—manual commit advised.")
  
    end_time = time.time()
    runtime = end_time - start_time
    print(f"\n--- CASCADE COMPLETE: {len(total_changes)} references annihilated across {file_count} files. Time: {runtime:.1f}s ---")
    if eta:
        total_matches = sum(int(c.split(':')[1].split()[0]) for c in total_changes if 'ref' in c)
        print(f" - Summary: {processed} files purged, {total_matches} ghosts culled.")
  
    if new_dir:
        clone_path = root_path / new_dir
        clone_path.mkdir(exist_ok=True)
        for item in root_path.iterdir():
            if item.is_dir():
                shutil.copytree(item, clone_path / item.name, dirs_exist_ok=True)
            else:
                shutil.copy2(item, clone_path)
        print(f"✓ Cloned to shadow dir: {new_dir}")
        root_path = clone_path
  
    if bundle:
        bundle_name = f"{root_path.name}_clean_{random.randint(1000,9999)}.zip"
        shutil.make_archive(bundle_name, 'zip', root_path)
        print(f"✓ Bundled payload: {bundle_name}—ready for dark drop.")
  
    # Clean temps - Fixed glob for absolute/relative wildcards via os.glob
    temp_patterns = [__file__]
    if not backup:
        temp_patterns.extend(['/tmp/*replace*', f"{root_path.name}_backup_*.zip"])
    else:
        temp_patterns.append('/tmp/*replace*')
    for tf in temp_patterns:
        if '*' in tf:
            glob_matches = os.glob(tf)
            for f_str in glob_matches:
                f_path = Path(f_str)
                if f_path.exists():
                    shred_file(f_path, verbose)
        else:
            f_path = Path(tf)
            if f_path.exists():
                shred_file(f_path, verbose)
  
    if log_file:
        with open(log_file, 'a') as lf:  # Append for daemon
            json.dump(log_entries, lf, indent=2)
        print(f"Log etched in void: {log_file}")

def main():
    parser = argparse.ArgumentParser(description="ShadowVault Code Purifier: Dynamic watermark hunter—scan first, purge if ghosts lurk.")
    parser.add_argument('path', nargs='?', default='.', type=str, help='Dir to clean (default: current)')
    parser.add_argument('--dry-run', '-d', action='store_true', help='Preview only')
    parser.add_argument('--mutate', '-m', action='store_true', help='Add neutral variations')
    parser.add_argument('--commit', '-c', action='store_true', help='Auto-git commit/push')
    parser.add_argument('--bundle', '-b', action='store_true', help='ZIP output')
    parser.add_argument('--backup', action='store_true', help='Pre-clean ZIP')
    parser.add_argument('--verbose', '-v', action='store_true', help='Detailed progress')
    parser.add_argument('--new-dir', '-n', type=str, help='Clone to new subdir')
    parser.add_argument('--log-file', '-l', type=str, help='Export JSON log')
    parser.add_argument('--eta', action='store_true', help='Show ETA/summary')
    # New: Multi-Tool Obfuscation
    parser.add_argument('--obfuscate', '-o', action='store_true', help='Obfuscate JS/CSS post-scrub (requires uglifyjs/cleancss)')
    # New: Vuln Scanner
    parser.add_argument('--vuln-scan', '-s', action='store_true', help='Run vuln scans post-scrub')
    parser.add_argument('--auto-patch', '-a', action='store_true', help='Auto-patch detected vulns (with --vuln-scan)')
    # New: Daemon Mode
    parser.add_argument('--daemon', action='store_true', help='Run as background daemon (hourly sweeps; requires python-daemon)')
    # New: Branch Rename
    parser.add_argument('--rename-branches', '-r', action='store_true', help='Rename watermark-tainted Git branches to neutral decoys')
    # New: Dynamic Patterns
    parser.add_argument('--pattern-file', type=str, help='JSON file for custom/dynamic watermark patterns')
    args = parser.parse_args()
  
    # Dynamic pattern load
    patterns = load_patterns(args.pattern_file)
  
    root = Path(args.path).resolve()
    if not root.exists():
        sys.exit("Path not found.")
  
    print(f"ShadowVault awakening in {root}—scanning voids for AI ghosts...")
    
    if args.daemon:
        if not DAEMON_AVAILABLE:
            print("Falling back to threaded daemon (install python-daemon for full isolation).")
            daemon_loop(root, args.dry_run, args.mutate, args.commit, args.bundle, args.backup, 
                       args.verbose, args.new_dir, args.log_file, args.eta, args.obfuscate, args.vuln_scan, args.auto_patch, args.rename_branches, args.pattern_file)
        else:
            with daemon.DaemonContext():
                daemon_loop(root, args.dry_run, args.mutate, args.commit, args.bundle, args.backup, 
                           args.verbose, args.new_dir, args.log_file, args.eta, args.obfuscate, args.vuln_scan, args.auto_patch, args.rename_branches, args.pattern_file)
    else:
        walk_and_scrub(root, args.dry_run, args.mutate or not args.dry_run, args.commit, args.bundle, 
                      args.backup, args.verbose, args.new_dir, args.log_file, args.eta, 
                      args.obfuscate, args.vuln_scan, args.auto_patch, args.rename_branches, patterns)
  
    sys.exit(0)

if __name__ == "__main__":
    main()