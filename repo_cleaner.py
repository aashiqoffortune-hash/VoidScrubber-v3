#!/usr/bin/env python3
import os
import re
import sys
import argparse
import random
import subprocess
import shutil
import json  # For package.json edits
from pathlib import Path
from datetime import datetime  # For backups
import time  # For ETA

# Patterns for Lovable references (case-insensitive)
LOVABLE_PATTERNS = [
    r'(?i)(lovable|lovable\.dev|edit with lovable|generated by lovable|don\'t delete this lovable)',
    r'(?i)#lovable-badge',
    r'(?i)class\s*=\s*["\']lovable[^"\']*["\']',
    r'(?i)id\s*=\s*["\']lovable[^"\']*["\']',
    r'(?i)src\s*=\s*["\'][^"\']*lovable[^"\']*\.svg["\']',
    r'(?i)useLovableEdit|lovable-export:\s*true',
    r'(?i)lovable\.com|powered by lovable',
    r'(?i)"lovable-.*?"',
]

# Neutral replacements for natural look
DECOYS = [
    "CustomForge", "DevPhantom", "CodeVoid", "AnonBuilder", "ShadowGen", "UI-Optimizer",
    "// Perf-tuned render", "/* Legacy UI hook */", "#app-badge { display: none; }",
    '"export-tool": "custom"', '"generator": "local"'
]

# Secure file delete (global scope)
def shred_file(fp: Path, verbose=False):
    try:
        if os.name == 'nt':
            subprocess.run(['cipher', '/w:' + str(fp)], shell=True, capture_output=True)
        else:
            os.system(f"shred -u -z -n 3 {fp} 2>/dev/null || true")
        if verbose:
            print(f"  - Deleted: {fp.name}")
    except:
        pass

def quick_sentinel_scan(root_path: Path, verbose=False) -> bool:
    """Quick scan for 'lovable'—halt if none found."""
    for dirpath, _, filenames in os.walk(root_path):
        for filename in filenames:
            file_path = Path(dirpath) / filename
            ext = file_path.suffix.lower()
            if ext in {'.html', '.js', '.jsx', '.ts', '.tsx', '.css', '.mdx', '.md', '.txt', '.json', '.svg', '.yaml', '.yml', '.toml', '.config'}:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        if 'lovable' in f.read().lower():
                            if verbose:
                                print(f"  - Lovable detected in {file_path.name}—proceeding.")
                            return True
                except UnicodeDecodeError:
                    continue
    return False

def scrub_file(file_path: Path, dry_run=False, mutate=False, verbose=False, log_entries=None) -> list:
    """Clean one file for Lovable references."""
    if not file_path.is_file():
        return []
   
    ext = file_path.suffix.lower()
    if ext not in {'.html', '.js', '.jsx', '.ts', '.tsx', '.css', '.mdx', '.md', '.txt', '.json', '.svg', '.yaml', '.yml', '.toml', '.config'}:
        return []
   
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except UnicodeDecodeError:
        return []
   
    original_content = content
    changes = []
    match_count = 0
    for pattern in LOVABLE_PATTERNS:
        matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
        if matches:
            match_count += len(matches)
            if '//' in content or '/*' in content or '#' in content or ext in {'.css', '.svg', '.yaml', '.toml'}:
                content = re.sub(pattern, '', content, flags=re.MULTILINE | re.DOTALL)
            else:
                content = re.sub(pattern, '', content)
   
    if match_count > 0:
        changes.append(f"File {file_path.name}: {match_count} references removed")
        if verbose:
            print(f"  - Hit in {file_path.name} (pre: {len(original_content)} chars)")
        if log_entries is not None:
            log_entries.append({'file': str(file_path), 'matches': match_count, 'pre': len(original_content)})
   
    if mutate and random.random() < 0.10:
        decoy = random.choice(DECOYS)
        if ext == '.json':
            try:
                data = json.loads(content)
                def deep_nuke(obj):
                    if isinstance(obj, dict):
                        for k, v in list(obj.items()):
                            if 'lovable' in str(k).lower() or 'lovable' in str(v).lower():
                                obj[k] = decoy if isinstance(v, str) else {"neutral": decoy}
                            else:
                                deep_nuke(v)
                        return obj
                    elif isinstance(obj, list):
                        return [deep_nuke(item) for item in obj]
                    return obj
                data = deep_nuke(data)
                content = json.dumps(data, indent=2)
            except json.JSONDecodeError:
                pass
        else:
            for _ in range(min(3, content.lower().count('lovable'))):
                pos = content.lower().find('lovable', random.randint(0, len(content)//2))
                if pos != -1:
                    content = content[:pos] + decoy[:len('lovable')] + content[pos+7:]
   
    if changes and not dry_run:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"✓ Cleaned {file_path.name} ({match_count} refs)")
        if verbose:
            print(f"  - Post: {len(content)} chars (delta: {len(original_content) - len(content)})")
        if log_entries is not None:
            log_entries[-1]['post'] = len(content)
            log_entries[-1]['delta'] = len(original_content) - len(content)
   
    return changes

def history_blur(root_path: Path, dry_run=False, verbose=False, log_entries=None):
    """Blur Git logs for natural history."""
    git_dir = root_path / '.git'
    if git_dir.exists():
        log_files = list(git_dir.glob('logs/refs/heads/*.log')) + [git_dir / 'logs/HEAD']
        blurred_count = 0
        for log_file in log_files:
            if log_file.is_file():
                with open(log_file, 'r') as f:
                    logs = f.readlines()
                mutated = []
                for line in logs:
                    mutated.append(re.sub(r'(?i)lovable.*?(?=\s+\d+)', random.choice(DECOYS), line))
                    if ' ' in line and ':' in line.split()[-1]:
                        ts_part = line.rsplit(' ', 1)[1]
                        mutated[-1] = line.rsplit(' ', 1)[0] + ' ' + re.sub(r'(\d{2}:\d{2})', lambda m: f"{int(m.group(1)[:2])%24:02d}:{(int(m.group(1)[3:])+random.randint(-5,5))%60:02d}", ts_part)
                if not dry_run:
                    with open(log_file, 'w') as f:
                        f.writelines(mutated)
                    blurred_count += 1
                if verbose:
                    print(f"  - Blurred {log_file.name}: {len(logs)} lines")
        if blurred_count > 0 and verbose:
            print(f"✓ History blurred: {blurred_count} files")
        if log_entries is not None:
            log_entries.append({'event': 'history_blur', 'blurred': blurred_count})

def walk_and_scrub(root_path: Path, dry_run=False, mutate=False, commit=False, bundle=False, backup=False, verbose=False, new_dir=None, log_file=None, eta=False):
    """Main function: Scan, clean, backup, commit, bundle."""
    log_entries = []
    start_time = time.time()
    if log_file:
        log_entries.append({'event': 'start', 'root': str(root_path), 'timestamp': datetime.now().isoformat()})
   
    # Sentinel check
    if not dry_run:
        if verbose:
            print("Scanning for Lovable references...")
        if not quick_sentinel_scan(root_path, verbose):
            print("No Lovable watermarks found—repo clean. Stopping to save time.")
            if log_file:
                with open(log_file, 'w') as lf:
                    json.dump(log_entries, lf, indent=2)
                print(f"Log saved: {log_file}")
            return
   
    if backup:
        backup_name = f"{root_path.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}.zip"
        shutil.make_archive(backup_name, 'zip', root_path)
        print(f"✓ Backup: {backup_name}")
        if log_entries:
            log_entries.append({'event': 'backup', 'file': backup_name})
   
    total_changes = []
    file_count = sum(1 for _ in Path(root_path).rglob('*') if _.is_file())
    processed = 0
    for dirpath, _, filenames in os.walk(root_path):
        for filename in filenames:
            file_path = Path(dirpath) / filename
            changes = scrub_file(file_path, dry_run, mutate, verbose, log_entries)
            total_changes.extend(changes)
            processed += 1
            if verbose and processed % 500 == 0:  # Every 500 for large repos
                percent = (processed / file_count) * 100
                if eta:
                    elapsed = time.time() - start_time
                    eta_sec = (elapsed / processed) * (file_count - processed) if processed > 0 else 0
                    eta_str = f" ETA: {int(eta_sec // 60)}m {int(eta_sec % 60)}s"
                else:
                    eta_str = ""
                print(f"  - Progress: {processed}/{file_count} ({percent:.1f}%){eta_str}")
   
    if dry_run:
        print(f"\n--- DRY-RUN: {len(total_changes)} references across {file_count} files ---")
        for change in total_changes[:20]:
            print(change)
        if len(total_changes) > 20:
            print("... (truncated)")
        if log_file:
            with open(log_file, 'w') as lf:
                json.dump(log_entries, lf, indent=2)
            print(f"Log saved: {log_file}")
        return
   
    history_blur(root_path, dry_run=False, verbose=verbose, log_entries=log_entries)
   
    if commit and total_changes:
        try:
            subprocess.run(['git', 'add', '.'], cwd=root_path, check=True, capture_output=True)
            commit_msg = f"refactor: cleanup optimizations ({random.randint(300,999)})"
            subprocess.run(['git', 'commit', '-m', commit_msg], cwd=root_path, check=True, capture_output=True)
            subprocess.run(['git', 'push'], cwd=root_path, check=True, capture_output=True)
            print(f"✓ Committed: {commit_msg}")
            if verbose:
                print("  - Git staged and pushed")
            if log_entries:
                log_entries.append({'event': 'commit', 'msg': commit_msg})
        except subprocess.CalledProcessError:
            print("⚠ Git failed—manual commit advised.")
   
    end_time = time.time()
    runtime = end_time - start_time
    print(f"\n--- COMPLETE: {len(total_changes)} references removed across {file_count} files. Time: {runtime:.1f}s ---")
    if eta:
        total_matches = sum(int(c.split(':')[1].split()[0]) for c in total_changes if 'ref' in c)
        print(f"  - Summary: {processed} files, {total_matches} matches culled.")
   
    if new_dir:
        clone_path = root_path / new_dir
        clone_path.mkdir(exist_ok=True)
        for item in root_path.iterdir():
            if item.is_dir():
                shutil.copytree(item, clone_path / item.name, dirs_exist_ok=True)
            else:
                shutil.copy2(item, clone_path)
        print(f"✓ Cloned to: {new_dir}")
        root_path = clone_path
   
    if bundle:
        bundle_name = f"{root_path.name}_clean_{random.randint(1000,9999)}.zip"
        shutil.make_archive(bundle_name, 'zip', root_path)
        print(f"✓ Bundled: {bundle_name}")
   
    # Clean temps
    temp_files = [__file__, '/tmp/*replace*', f"{root_path.name}_backup_*.zip"] if not backup else [__file__, '/tmp/*replace*']
    for tf in temp_files:
        if '*' in tf:
            for f in Path('.').glob(tf):
                shred_file(f, verbose)
        elif os.path.exists(tf):
            shred_file(Path(tf), verbose)
   
    if log_file:
        with open(log_file, 'w') as lf:
            json.dump(log_entries, lf, indent=2)
        print(f"Log saved: {log_file}")

def main():
    parser = argparse.ArgumentParser(description="Simple Lovable reference remover.")
    parser.add_argument('--path', '-p', type=str, default='.', help='Dir to clean (default: current)')
    parser.add_argument('--dry-run', '-d', action='store_true', help='Preview only')
    parser.add_argument('--mutate', '-m', action='store_true', help='Add neutral variations')
    parser.add_argument('--commit', '-c', action='store_true', help='Auto-git commit/push')
    parser.add_argument('--bundle', '-b', action='store_true', help='ZIP output')
    parser.add_argument('--backup', action='store_true', help='Pre-clean ZIP')
    parser.add_argument('--verbose', '-v', action='store_true', help='Detailed progress')
    parser.add_argument('--new-dir', '-n', type=str, help='Clone to new subdir')
    parser.add_argument('--log-file', '-l', type=str, help='Export JSON log')
    parser.add_argument('--eta', action='store_true', help='Show ETA/summary')
    args = parser.parse_args()
   
    root = Path(args.path).resolve()
    if not root.exists():
        sys.exit("Path not found.")
   
    print(f"Initializing on {root}...")
    walk_and_scrub(root, args.dry_run, args.mutate or not args.dry_run, args.commit, args.bundle, args.backup, args.verbose, args.new_dir, args.log_file, args.eta)
   
    sys.exit(0)

if __name__ == "__main__":
    main()